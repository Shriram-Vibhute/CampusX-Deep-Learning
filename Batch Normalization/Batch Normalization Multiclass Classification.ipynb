{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Internal Covariate Shift: The \"Shifting Sand\" Problem in Neural Networks\n",
        "\n",
        "Imagine you're teaching a group of students to solve math problems. Every day, you change the rules:\n",
        "- **Monday:** Problems use only positive numbers  \n",
        "- **Tuesday:** Problems suddenly include decimals  \n",
        "- **Wednesday:** Problems switch to algebra  \n",
        "\n",
        "Students struggle because the **foundation keeps shifting**. This is exactly what happens inside neural networks without batch normalization—it's called **Internal Covariate Shift (ICS)**.\n",
        "\n",
        "---\n",
        "\n",
        "### What is Internal Covariate Shift?\n",
        "**Definition:**  \n",
        "The change in *distribution of inputs* to deeper layers in a neural network caused by weight updates during training. Each layer constantly adapts to a moving target.\n",
        "\n",
        "#### Key Problem:\n",
        "- Layer 1 updates its weights → changes its output distribution  \n",
        "- This shifted output becomes Layer 2's input → Layer 2 must relearn  \n",
        "- Cascades through all layers → **slow/unstable training**\n",
        "\n",
        "---\n",
        "\n",
        "### Why It Happens: The Domino Effect\n",
        "Consider a 3-layer network processing images:\n",
        "1. **Initial state:**  \n",
        "   - Layer 1 outputs values centered around 0.5  \n",
        "   - Layer 2 learns weights for inputs ~0.5  \n",
        "\n",
        "2. **After weight update:**  \n",
        "   - Layer 1 now outputs values centered around 2.3  \n",
        "   - Layer 2's weights are **obsolete**—designed for 0.5, not 2.3!  \n",
        "\n",
        "3. **Result:**  \n",
        "   - Layer 2 scrambles to adjust → slows learning  \n",
        "   - Gradients become unstable (explode/vanish)  \n",
        " \n",
        "*`Distribution shift between layers during training`*\n",
        "\n",
        "---\n",
        "\n",
        "### How Batch Normalization Fixes ICS\n",
        "BatchNorm acts like a `stabilizer` between layers. Here's how it works for any layer:\n",
        "\n",
        "#### Step 1: Standardize Inputs (Per Mini-Batch)\n",
        "For each input feature to the layer:  \n",
        "- **Compute mean (μ) and variance (σ²)** across the batch  \n",
        "- **Normalize:**  \n",
        "  ```\n",
        "  x̂ = (x - μ) / √(σ² + ε)   // ε=tiny number for stability\n",
        "  ```  \n",
        "  → Forces inputs to mean=0, variance=1  \n",
        "\n",
        "#### Step 2: Learnable Scaling  \n",
        "- **Add flexibility:**  \n",
        "  ```\n",
        "  y = γ * x̂ + β  \n",
        "  ```  \n",
        "  - γ (gamma) scales the standard deviation  \n",
        "  - β (beta) shifts the mean  \n",
        "  → Allows the network to *choose* optimal distribution  \n",
        "\n",
        "---\n",
        "\n",
        "### Intuition Through Analogy\n",
        "**Without BatchNorm:**  \n",
        "- Like students solving problems on a wobbly desk (shifting inputs)  \n",
        "- Hard to focus—constantly adjusting to instability  \n",
        "\n",
        "**With BatchNorm:**  \n",
        "1. **Standardization:**  \n",
        "   - Nails the desk to the floor (stable foundation)  \n",
        "2. **Gamma/Beta:**  \n",
        "   - Lets students *tilt* the desk if needed (γ/β adapt to optimal angle)  \n",
        "\n",
        "→ Students now learn faster because they're not fighting instability  \n",
        "\n",
        "---\n",
        "\n",
        "### Why This Matters\n",
        "| **Problem**         | **BatchNorm Solution**       | **Effect**                  |\n",
        "|---------------------|------------------------------|-----------------------------|\n",
        "| Inputs shift wildly | Forces mean=0, variance=1    | Stable inputs to next layer |\n",
        "| Slow convergence    | Smoother loss landscape      | Faster training             |\n",
        "| Gradient explosion  | Controls value magnitudes    | -                           |\n",
        "\n",
        "---\n",
        "\n",
        "### Limitations & Tradeoffs\n",
        "- BatchNormalization works well for standard neural networks (ANNs) but faces challenges in Recurrent Neural Networks (RNNs) and Transformers. This is because RNNs process sequences where padding (adding zeros to make sequences equal length) is common. When applying BatchNorm feature-wise to padded sequences, the calculated mean and standard deviation become skewed due to these artificial zero values, making them poor representatives of the true data distribution.\n",
        "<br>\n",
        "Alternatives - `LayerNorm (RNNs), InstanceNorm (style transfer)` \n",
        "\n",
        "---\n",
        "\n",
        "### Why \"Covariate Shift\"?\n",
        "- **Covariate** = Input variable (e.g., pixel values)  \n",
        "- **Shift** = Change in statistical distribution  \n",
        "- **Internal** = Happening *inside* the network (not at raw input)  \n",
        "\n",
        "BatchNorm is the ultimate stabilizer—turning shifting sand into solid concrete for faster, deeper networks! 🏗️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "yytPWYIXPWDk"
      },
      "outputs": [],
      "source": [
        "# Importing Modules\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Input, BatchNormalization, Activation, Flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "082sdx7iPpfc"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "U-0NJDcZQCV1",
        "outputId": "9f34df15-3b53-4e1e-e24a-1fa2b7b40803"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_15\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_15\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_33               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)                   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_34               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_35               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m784\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_33               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m784\u001b[0m)                   │           \u001b[38;5;34m3,136\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m300\u001b[0m)                   │         \u001b[38;5;34m235,500\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_34               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m300\u001b[0m)                   │           \u001b[38;5;34m1,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m)                   │          \u001b[38;5;34m30,100\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_35               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m)                   │             \u001b[38;5;34m400\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m10\u001b[0m)                    │           \u001b[38;5;34m1,010\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">271,346</span> (1.04 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m271,346\u001b[0m (1.04 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,978</span> (1.03 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,978\u001b[0m (1.03 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,368</span> (9.25 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,368\u001b[0m (9.25 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Batch Normalization after activation\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input(shape = [28, 28], batch_size = 32))\n",
        "model.add(Flatten())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(units = 300, activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(units = 100, activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(units = 10, activation = 'sigmoid'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "PfWWe27JQhEs"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer = 'Adam',\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5GaTPf8U3oG",
        "outputId": "8558a316-67a8-4043-fa6b-db3914bc0aed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 10ms/step - accuracy: 0.8011 - loss: 0.5551 - val_accuracy: 0.8522 - val_loss: 0.4253\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.8632 - loss: 0.3706 - val_accuracy: 0.8764 - val_loss: 0.3703\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.8784 - loss: 0.3261 - val_accuracy: 0.8714 - val_loss: 0.3602\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.8887 - loss: 0.2969 - val_accuracy: 0.8801 - val_loss: 0.3444\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.8981 - loss: 0.2747 - val_accuracy: 0.8841 - val_loss: 0.3428\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9010 - loss: 0.2659 - val_accuracy: 0.8825 - val_loss: 0.3394\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9088 - loss: 0.2432 - val_accuracy: 0.8865 - val_loss: 0.3298\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.9112 - loss: 0.2370 - val_accuracy: 0.8911 - val_loss: 0.3711\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9175 - loss: 0.2225 - val_accuracy: 0.8885 - val_loss: 0.3441\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9213 - loss: 0.2094 - val_accuracy: 0.8885 - val_loss: 0.3765\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79756dd4bf40>"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train, y_train, batch_size = 32, epochs = 10, validation_data = (X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training vs. Inference: The Problem\n",
        "\n",
        "During **training**, BatchNorm calculates the mean (μ) and standard deviation (σ) **from the current mini-batch**. This is dynamic and changes with every batch.\n",
        "\n",
        "During **inference**, we don't have mini-batches in the same way (we might get a single sample, not a batch). We need a fixed, reliable estimate.\n",
        "\n",
        "### The Solution: Running (Population) Statistics\n",
        "\n",
        "Instead of using batch statistics, BatchNorm uses **running averages** of the mean and variance it observed during training.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1.  **During Training:** For every mini-batch, the layer:\n",
        "    *   Calculates the batch mean (μ_batch) and variance (σ²_batch).\n",
        "    *   **Updates the running averages** using these values:\n",
        "        *   `running_mean = momentum * running_mean + (1 - momentum) * μ_batch`\n",
        "        *   `running_variance = momentum * running_variance + (1 - momentum) * σ²_batch`\n",
        "    *   (Where `momentum` is a hyperparameter, typically close to 0.9 or 0.99, which controls how much to keep from the past).\n",
        "\n",
        "2.  **During Inference:** The layer **stops calculating new statistics**. It simply uses the final, learned `running_mean` and `running_variance` values (often called the \"population statistics\") to normalize *any* input it receives.\n",
        "    *   `normalized_input = (input - running_mean) / sqrt(running_variance + ε)`\n",
        "\n",
        "### In Short:\n",
        "\n",
        "| Phase | How it gets Mean & Std | Why? |\n",
        "| :--- | :--- | :--- |\n",
        "| **Training** | From the **current mini-batch**. Also updates **running averages**. | To stabilize training and learn the statistics. |\n",
        "| **Inference** | From the final **running averages** learned during training. | To get a **stable, fixed** normalization for any input, ensuring consistent behavior. |\n",
        "\n",
        "This ensures the model's behavior is consistent and deterministic after it's deployed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "hTgsodx4VIIu",
        "outputId": "c6f26ae8-2ad4-4c98-e03e-99d860b79dc9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_16\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_16\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_36               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)                   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_37               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_38               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m784\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_36               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m784\u001b[0m)                   │           \u001b[38;5;34m3,136\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_51 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m300\u001b[0m)                   │         \u001b[38;5;34m235,500\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_37               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m300\u001b[0m)                   │           \u001b[38;5;34m1,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m300\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_52 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m)                   │          \u001b[38;5;34m30,100\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_38               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m)                   │             \u001b[38;5;34m400\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_53 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m10\u001b[0m)                    │           \u001b[38;5;34m1,010\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">271,346</span> (1.04 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m271,346\u001b[0m (1.04 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,978</span> (1.03 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m268,978\u001b[0m (1.03 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,368</span> (9.25 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,368\u001b[0m (9.25 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Batch Normalization before activation\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Input(shape = [28, 28], batch_size = 32))\n",
        "model.add(Flatten())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(units = 300))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(units = 100))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(units = 10, activation = 'sigmoid'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "OLqs3kboZD7U"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer = 'Adam',\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPzt8dDsZGgI",
        "outputId": "d0006d78-cb6b-45ad-df54-ac3de19e7080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.8073 - loss: 0.5515 - val_accuracy: 0.8613 - val_loss: 0.3913\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.8689 - loss: 0.3511 - val_accuracy: 0.8735 - val_loss: 0.3513\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - accuracy: 0.8826 - loss: 0.3147 - val_accuracy: 0.8787 - val_loss: 0.3392\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.8926 - loss: 0.2921 - val_accuracy: 0.8811 - val_loss: 0.3294\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - accuracy: 0.8976 - loss: 0.2746 - val_accuracy: 0.8828 - val_loss: 0.3279\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.9016 - loss: 0.2634 - val_accuracy: 0.8853 - val_loss: 0.3542\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - accuracy: 0.9111 - loss: 0.2365 - val_accuracy: 0.8917 - val_loss: 0.3311\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.9164 - loss: 0.2248 - val_accuracy: 0.8910 - val_loss: 0.3397\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 11ms/step - accuracy: 0.9184 - loss: 0.2168 - val_accuracy: 0.8948 - val_loss: 0.3550\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 13ms/step - accuracy: 0.9226 - loss: 0.2046 - val_accuracy: 0.8936 - val_loss: 0.3540\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79756aaa6d10>"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train, y_train, batch_size = 32, epochs = 10, validation_data = (X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Problem: The \"Forced\" Normalization\n",
        "\n",
        "Remember what the first step of BatchNorm does:\n",
        "1.  It takes a batch of data for a layer.\n",
        "2.  It calculates the mean (`μ`) and variance (`σ²`) of that batch.\n",
        "3.  It **forces** the data to have a standard normal distribution: `mean = 0`, `variance = 1`.\n",
        "    *   `x_hat = (x - μ) / √(σ² + ε)`\n",
        "\n",
        "**But here's the critical issue:** What if a standard normal distribution (mean 0, variance 1) isn't the *best* distribution for the data to have for the network to learn? What if the optimal distribution has a different mean and variance?\n",
        "\n",
        "For example, if you use a **sigmoid** activation function after the BatchNorm layer, its most \"active\" and useful region is *not* around 0. For sigmoid, the linear region is roughly between -2 and 2. For a **ReLU** activation, the optimal distribution might be centered slightly positively.\n",
        "\n",
        "By *forcing* the data to mean 0 and variance 1, we might be:\n",
        "*   **Removing useful information** from the previous layer.\n",
        "*   **Slowing down learning** by making the activation functions operate in their less sensitive regions.\n",
        "\n",
        "---\n",
        "\n",
        "### The Solution: The \"Learnable Undo\" (Gamma and Beta)\n",
        "\n",
        "This is where `γ` (gamma) and `β` (beta) come in. They are **learnable parameters** that give the network two powerful knobs to turn:\n",
        "\n",
        "1.  **Beta (β)**: The **Shift** parameter (controls the mean)\n",
        "    *   It adds a learned value to the normalized data: `y = γ * x_hat + β`\n",
        "    *   If the network learns that the data should have a mean of `0.5` instead of `0`, it can set `β = 0.5`.\n",
        "    *   **Analogy:** After the BatchNorm \"machine\" centers the data at 0, `β` says, \"Actually, let's shift everything to the right by 0.5.\"\n",
        "\n",
        "2.  **Gamma (γ)**: The **Scale** parameter (controls the variance)\n",
        "    *   It multiplies the normalized data by a learned value.\n",
        "    *   If the network learns that the data should have a wider spread (variance > 1), it can set `γ` to a value greater than 1 (e.g., `γ = 1.5`).\n",
        "    *   If it needs a tighter spread (variance < 1), it can set `γ` to a value less than 1 (e.g., `γ = 0.5`).\n",
        "    *   **Analogy:** After the BatchNorm \"machine\" squishes the data to a standard spread, `γ` says, \"Actually, let's stretch it out a bit more\" or \"Let's squish it even further.\"\n",
        "\n",
        "The final, brilliant output of a BatchNorm layer is:\n",
        "**`y = γ * x_hat + β`**\n",
        "\n",
        "This `y` is what gets passed to the next layer or the activation function.\n",
        "\n",
        "---\n",
        "\n",
        "### A Simple Analogy: The Perfect Fit Tailor\n",
        "\n",
        "Imagine Batch Normalization is a **standard-sized clothing rack**. All clothes are forced to be a single, average size (Mean 0, Variance 1). This is better than having wildly different sizes, but it won't fit anyone perfectly.\n",
        "\n",
        "*   **Gamma (γ)** is like taking the standard-sized shirt and **letting out the seams or taking them in** to adjust how *loose* or *tight* it is (the scale/variance).\n",
        "*   **Beta (β)** is like **adding padding to the shoulders or shortening the sleeves** to adjust where the shirt sits on the body (the shift/mean).\n",
        "\n",
        "The network is the \"customer.\" During training, it learns the perfect `γ` and `β` for each batch norm layer, \"tailoring\" the standard-sized output to its exact needs.\n",
        "\n",
        "### Why This is So Important: Flexibility and Representation Power\n",
        "\n",
        "1.  **The Network Can \"Undo\" BatchNorm:**\n",
        "    This is the most important point. If the network discovers that normalizing the data isn't helpful for a particular layer, it can easily learn to reverse it.\n",
        "    *   It can set `γ = √(σ² + ε)` and `β = μ`\n",
        "    *   The output then becomes: `y = √(σ² + ε) * x_hat + μ = x`\n",
        "    *   **It perfectly reconstructs the original input!** This means that if batch normalization isn't useful, the network can learn to effectively turn it off. This makes adding BatchNorm a virtually risk-free operation.\n",
        "\n",
        "2.  **Preserves Non-Linearity:**\n",
        "    The network can learn the optimal distribution to feed into the next non-linear activation function (Sigmoid, ReLU, etc.), ensuring the model's expressive power is maintained and even enhanced.\n",
        "\n",
        "3.  **Learns the Optimal Data Distribution:**\n",
        "    Instead of being forced into a one-size-fits-all distribution, each BatchNorm layer learns the best possible distribution for the task at hand, leading to faster and better learning.\n",
        "\n",
        "### Summary Table\n",
        "\n",
        "| Parameter | Symbol | What it Does | Analogy | If it learns... |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **Beta** | `β` | **Shifts** the data. Controls the **mean**. | Adjusting the center point. | `β = 0.5` -> New mean is 0.5. |\n",
        "| **Gamma** | `γ` | **Scales** the data. Controls the **variance**. | Adjusting the spread. | `γ = 2.0` -> New variance is 4.0. |\n",
        "\n",
        "**In a nutshell:** `γ` and `β` transform Batch Normalization from a rigid, forced standardization into a flexible, learnable layer. They allow the network to **learn the ideal mean and variance** for the normalized data at each layer, balancing the stability of normalization with the expressive power needed for complex learning. This is why they are indispensable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
