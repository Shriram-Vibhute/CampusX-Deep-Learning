{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "W_bMe-pYipZ_"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Dense, SimpleRNN, Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "U2v0WPuPjRS_"
      },
      "outputs": [],
      "source": [
        "# Dummy data\n",
        "docs = [\n",
        "    'go india',\n",
        "    'india india',\n",
        "    'hip hip hurray',\n",
        "    'jeetega bhai jeetega india jeetega',\n",
        "    'bharat mata ki jai',\n",
        "    'kohli kohli',\n",
        "    'sachin sachin',\n",
        "    'dhoni dhoni',\n",
        "    'modi ji ki jai',\n",
        "    'inquilab zindabad',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*   **`num_words=None`**:\n",
        "    *   This parameter controls the maximum number of words to keep, based on their frequency.\n",
        "    *   If set to an integer (e.g., `num_words=10000`), the tokenizer will only consider the top `10000` most frequent words in your corpus. Words beyond this count will be discarded.\n",
        "    *   If set to `None` (as in your example), it means **all words** found in the corpus will be kept and indexed. This can lead to a very large vocabulary if your text data is extensive.\n",
        "\n",
        "*   **`filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'`**:\n",
        "    *   This is a string containing characters that will be **filtered out** from your text before tokenization.\n",
        "    *   By default, it includes most punctuation marks, tabs (`\\t`), and newlines (`\\n`).\n",
        "    *   The tokenizer will remove these characters from your input text, effectively cleaning it up before splitting it into words. For example, \"Hello, world!\" would become \"Hello world\" after filtering.\n",
        "\n",
        "*   **`lower=True`**:\n",
        "    *   This boolean parameter determines whether to convert text to **lowercase** before tokenization.\n",
        "    *   If `True` (as here), all text will be converted to lowercase. This is a common practice in NLP to treat \"The\" and \"the\" as the same word, reducing vocabulary size and improving consistency.\n",
        "    *   If `False`, the case of the words will be preserved.\n",
        "\n",
        "*   **`split=' '`**:\n",
        "    *   This parameter specifies the **delimiter** used to split text into individual words (tokens).\n",
        "    *   By default, it's a single space (`' '`), meaning words will be separated by spaces.\n",
        "    *   You could change this if your text uses a different primary delimiter, but for most natural language, a space is appropriate.\n",
        "\n",
        "*   **`char_level=False`**:\n",
        "    *   This boolean parameter controls whether tokenization should happen at the **character level** or **word level**.\n",
        "    *   If `False` (as here), the tokenizer will operate at the word level, treating sequences of characters separated by `split` characters (and after `filters` are applied) as individual tokens.\n",
        "    *   If `True`, each character in the input text would be treated as a separate token. This is less common for general text processing but can be useful for tasks like character-level language modeling or certain types of sequence-to-sequence problems.\n",
        "\n",
        "*   **`oov_token=None`**:\n",
        "    *   `oov` stands for \"Out-Of-Vocabulary\". This parameter allows you to specify a token that will represent words not found in the tokenizer's vocabulary (i.e., words that were not among the `num_words` most frequent words or were not seen during `fit_on_texts`).\n",
        "    *   If set to `None` (as here), out-of-vocabulary words will simply be **ignored** (removed) when converting text to sequences of integers.\n",
        "    *   If you provide a string (e.g., `oov_token='<unk>'`), any word not in the vocabulary will be replaced by this token. This is useful for ensuring that all input sequences have the same length and that your model can handle unseen words gracefully.\n",
        "\n",
        "*   **`analyzer=None`**:\n",
        "    *   This parameter is typically used for more advanced text processing, allowing you to specify a custom function to preprocess each document before tokenization.\n",
        "    *   If `None` (as here), the default Keras text processing (applying `filters`, `lower`, and `split`) will be used.\n",
        "    *   You could provide a callable (a function) here that takes a string as input and returns a list of tokens. This gives you fine-grained control over the tokenization process, for example, if you wanted to implement stemming or lemmatization before tokenizing.\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZEZY6h09ja-o"
      },
      "outputs": [],
      "source": [
        "# Tokenizing\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=None,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True,\n",
        "    split=' ',\n",
        "    char_level=False,\n",
        "    oov_token=\"<none>\",\n",
        "    analyzer=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fitting on documents\n",
        "tokenizer.fit_on_texts(docs) # Cannot handle null values by default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BybQ26ijk0qj",
        "outputId": "ea33605e-16b9-41fa-a92e-e920292a4d0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<none>': 1,\n",
              " 'india': 2,\n",
              " 'jeetega': 3,\n",
              " 'hip': 4,\n",
              " 'ki': 5,\n",
              " 'jai': 6,\n",
              " 'kohli': 7,\n",
              " 'sachin': 8,\n",
              " 'dhoni': 9,\n",
              " 'go': 10,\n",
              " 'hurray': 11,\n",
              " 'bhai': 12,\n",
              " 'bharat': 13,\n",
              " 'mata': 14,\n",
              " 'modi': 15,\n",
              " 'ji': 16,\n",
              " 'inquilab': 17,\n",
              " 'zindabad': 18}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Encoding of unique words in the documents\n",
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T6pdAyEk3Op",
        "outputId": "35df16e2-f522-44d8-e2d9-38fb628e0905"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('go', 1),\n",
              "             ('india', 4),\n",
              "             ('hip', 2),\n",
              "             ('hurray', 1),\n",
              "             ('jeetega', 3),\n",
              "             ('bhai', 1),\n",
              "             ('bharat', 1),\n",
              "             ('mata', 1),\n",
              "             ('ki', 2),\n",
              "             ('jai', 2),\n",
              "             ('kohli', 2),\n",
              "             ('sachin', 2),\n",
              "             ('dhoni', 2),\n",
              "             ('modi', 1),\n",
              "             ('ji', 1),\n",
              "             ('inquilab', 1),\n",
              "             ('zindabad', 1)])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Frequency of each word in the document\n",
        "tokenizer.word_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKDaC2-fk5KL",
        "outputId": "e6c43209-a2ec-4b63-fa7e-f2ea029b42b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# No. of documents(sentences) in the dataset\n",
        "tokenizer.document_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjujhwZnk61P",
        "outputId": "3e6609c7-9158-457f-d6ae-7ee88ef7025a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[10, 2],\n",
              " [2, 2],\n",
              " [4, 4, 11],\n",
              " [3, 12, 3, 2, 3],\n",
              " [13, 14, 5, 6],\n",
              " [7, 7],\n",
              " [8, 8],\n",
              " [9, 9],\n",
              " [15, 16, 5, 6],\n",
              " [17, 18]]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Encoding the documents\n",
        "sequences = tokenizer.texts_to_sequences(docs)\n",
        "sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rua18Cek8gQ",
        "outputId": "65438633-427b-4427-a889-f5451aed33d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[10,  2,  0,  0,  0],\n",
              "       [ 2,  2,  0,  0,  0],\n",
              "       [ 4,  4, 11,  0,  0],\n",
              "       [ 3, 12,  3,  2,  3],\n",
              "       [13, 14,  5,  6,  0],\n",
              "       [ 7,  7,  0,  0,  0],\n",
              "       [ 8,  8,  0,  0,  0],\n",
              "       [ 9,  9,  0,  0,  0],\n",
              "       [15, 16,  5,  6,  0],\n",
              "       [17, 18,  0,  0,  0]], dtype=int32)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Applying post zero padding to equalize the length of all the documents\n",
        "padded_sequences = pad_sequences(sequences, padding = 'post')\n",
        "padded_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embeddings:\n",
        "1.  **`Input`:** It takes **integer-encoded words** (e.g., \"cat\" is 5, \"dog\" is 12), not one-hot encodings. This is a key difference.\n",
        "2.  **`Lookup Table`:** It acts like a **lookup table**. For each integer input, it retrieves a corresponding dense vector (embedding).\n",
        "3.  **`Learned Vectors`:** These dense vectors are **learnable parameters** of the model. During training, the model adjusts these vectors so that words with similar meanings or contexts end up with similar embedding vectors.\n",
        "4.  **`Output`:** It outputs a sequence of these dense vectors, one for each word in the input sequence.   \n",
        "\n",
        "### Key Benefits of Embeddings:\n",
        "1. Captures the semantic meaning of words (multiple dimensions (aspects) per word)\n",
        "2. Trained through backpropagation - learns from actual data\n",
        "3. Flexible dimensionality - can adjust embedding size as needed\n",
        "\n",
        "`Note` - Choose your text processing method based on your task:\n",
        "- Use Embeddings for: text generation, summarization, and question answering\n",
        "- Use BOW or TF-IDF for: recommender systems and span detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "52W5_Dfzk_pd",
        "outputId": "5c1faf7d-0c7e-4c68-eee3-3bdc18ea29b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
              "array([[-0.08723129,  0.05023347],\n",
              "       [ 0.0454735 ,  0.08788645],\n",
              "       [ 0.03264703,  0.0687113 ],\n",
              "       [ 0.03264703,  0.0687113 ],\n",
              "       [ 0.03264703,  0.0687113 ]], dtype=float32)>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Embeddings from keras\n",
        "embedding = Embedding(\n",
        "        input_dim = 19,\n",
        "        output_dim = 2,\n",
        "        embeddings_initializer = \"normal\",\n",
        "        embeddings_regularizer = None,\n",
        "        embeddings_constraint = None,\n",
        "        mask_zero = False,\n",
        "        weights = None,\n",
        "        lora_rank = None,\n",
        "    )\n",
        "\n",
        "# Embeddings for first document\n",
        "embedding(padded_sequences[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
