{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867e76a3",
   "metadata": {},
   "source": [
    "### Max-Norm Regularization Explained Simply\n",
    "\n",
    "*   **How it works:** After each weight update step during training, you check every neuron's incoming weight vector. If its norm (like its \"magnitude\") exceeds your pre-defined limit `c`, you **scale the entire vector down** so that its norm is exactly `c`.\n",
    "*   **It's a hard constraint, not a soft penalty.** Unlike L1/L2 which adds a penalty to the loss function, Max-Norm directly imposes a ceiling on the size of the weights.\n",
    "\n",
    "**Simple Analogy: A Dog on a Leash**\n",
    "*   **Traditional L2 Regularization:** Is like constantly telling your dog, \"Don't pull too far ahead!\" (a soft penalty added to your overall walk). The dog might still lunge and strain.\n",
    "*   **Max-Norm Regularization:** Is like using a fixed-length leash. The dog can move freely within a circle around you, but it can **never** be further away than the length of the leash (a hard constraint).\n",
    "\n",
    "> In modern deep learning, **Max-Norm is often preferred** for complex architectures (like `RNNs` and `GANs`) due to its stability and effectiveness, while **L2 remains a solid, simple choice** for more standard networks (like `ANNs` and `CNNs`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8dd5b1",
   "metadata": {},
   "source": [
    "### 1. The Constraint (The Core Formula)\n",
    "\n",
    "For each neuron's incoming weight vector **w** (e.g., all weights connecting to a single neuron), the rule is enforced as follows:\n",
    "\n",
    "**If**  `||w||₂ > c`  \n",
    "**Then** `w ← (c / ||w||₂) * w`\n",
    "\n",
    "**Else** `w` remains unchanged.\n",
    "\n",
    "### 2. Breaking Down the Formula\n",
    "\n",
    "*   **`||w||₂`**: This is the **L2 norm** (the Euclidean length) of the weight vector **w**.\n",
    "    *   For a vector `w = (w₁, w₂, ..., wₙ)`, its L2 norm is `||w||₂ = √(w₁² + w₂² + ... + wₙ²)`.\n",
    "*   **`c`**: This is the hyperparameter you choose—the **maximum allowed norm**. Common values are between 1 and 4 (e.g., `c = 3.0`).\n",
    "*   **`(c / ||w||₂) * w`**: This is the **scaling operation**. If the norm of `w` exceeds `c`, we scale down the entire vector by the factor `c / ||w||₂`.\n",
    "\n",
    "### Simple Example\n",
    "\n",
    "Let's say:\n",
    "*   You set `c = 2.0`\n",
    "*   After a weight update, a neuron's weight vector is `w = [1.5, 2.0]`.\n",
    "\n",
    "1.  **Calculate its norm:** `||w||₂ = √(1.5² + 2.0²) = √(2.25 + 4.0) = √6.25 = 2.5`\n",
    "2.  **Check constraint:** `2.5 > 2.0` → The constraint is violated.\n",
    "3.  **Apply Max-Norm:** We scale the vector by `c / ||w||₂ = 2.0 / 2.5 = 0.8`\n",
    "    *   New weight vector: `w = 0.8 * [1.5, 2.0] = [1.2, 1.6]`\n",
    "4.  **Verify new norm:** `||w||₂ = √(1.2² + 1.6²) = √(1.44 + 2.56) = √4.0 = 2.0` ✅\n",
    "\n",
    "The direction of the vector was preserved, but its length was forcefully reduced to meet the constraint. This prevents any single neuron from becoming overly influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(\n",
    "    100, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "    kernel_constraint=tf.keras.constraints.max_norm(c=1.)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
